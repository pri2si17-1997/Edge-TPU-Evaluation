{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "NIH_14_DATASET_PATH = '../NIH_14/'\n",
    "dataset_path = os.path.abspath(NIH_14_DATASET_PATH)\n",
    "os.listdir(dataset_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "data_entry_csv_path = os.path.join(dataset_path, 'Data_Entry_2017.csv')\n",
    "data = pd.read_csv(data_entry_csv_path)\n",
    "print(f\"Data Shape : {data.shape}\")\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Removing patients with age greater than 100\n",
    "data = data[data['Patient Age']<100]\n",
    "\n",
    "print(f\"New dataset dimensions: {data.shape}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data[['Image Index', 'Finding Labels']]\n",
    "print(data.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob2\n",
    "\n",
    "all_images = sorted(glob2.glob(dataset_path + '/**/*.png'))\n",
    "print(f'Number of Images: {len(all_images)}')\n",
    "\n",
    "all_image_paths = {os.path.basename(x): x for x in all_images}\n",
    "\n",
    "#Add path of images as column to the dataset\n",
    "data['Path'] = data['Image Index'].map(all_image_paths.get)\n",
    "data.sample(5, random_state=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*data['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "print(all_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_labels = np.delete(all_labels, np.where(all_labels == 'No Finding'))\n",
    "all_labels = [x for x in all_labels]\n",
    "all_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        # Add a column for each desease\n",
    "        data[c_label] = data['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n",
    "        \n",
    "print(f\"Dataset Dimension: {data.shape}\")\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_counts = data['Finding Labels'].value_counts()\n",
    "label_counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data.groupby('Finding Labels').filter(lambda x : len(x)>11)\n",
    "label_counts = data['Finding Labels'].value_counts()\n",
    "print(label_counts.shape)\n",
    "print(label_counts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_and_valid_df, test_df = train_test_split(data,\n",
    "                                               test_size = 0.30,\n",
    "                                               random_state = 2018,\n",
    "                                              )\n",
    "\n",
    "train_df, valid_df = train_test_split(train_and_valid_df,\n",
    "                                      test_size=0.30,\n",
    "                                      random_state=2018,\n",
    "                                     )\n",
    "\n",
    "print(f'Training: {train_df.shape[0]} Validation: {valid_df.shape[0]} Testing: {test_df.shape[0]}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_generator = ImageDataGenerator(rescale=1./255)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "def flow_from_dataframe(image_generator, dataframe, batch_size):\n",
    "\n",
    "    df_gen = image_generator.flow_from_dataframe(dataframe,\n",
    "                                                 x_col='Path',\n",
    "                                                 y_col=all_labels,\n",
    "                                                 target_size=IMG_SIZE,\n",
    "                                                 classes=all_labels,\n",
    "                                                 color_mode='rgb',\n",
    "                                                 class_mode='raw',\n",
    "                                                 shuffle=False,\n",
    "                                                 batch_size=batch_size)\n",
    "    \n",
    "    return df_gen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                                dataframe= train_df,\n",
    "                                batch_size = 32)\n",
    "\n",
    "valid_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                                dataframe=valid_df,\n",
    "                                batch_size = 32)\n",
    "\n",
    "test_gen = flow_from_dataframe(image_generator=base_generator, \n",
    "                               dataframe=test_df,\n",
    "                               batch_size = 32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_x, train_y = next(train_gen)\n",
    "print(f\"Image Dimensions: {train_x[1].shape}\")\n",
    "print(f\"Labels: {train_y[1]}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape=(224, 224, 3)\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "base_model = DenseNet121(include_top=False, input_tensor=img_input, input_shape=input_shape, \n",
    "                         pooling=\"avg\", weights='imagenet')\n",
    "x = base_model.output\n",
    "predictions = Dense(len(all_labels), activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "model = Model(inputs=img_input, outputs=predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "    \n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "    \n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(\n",
    "    num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def apply_quantization_to_batch_normalization(layer):\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        return quantize_annotate_layer(layer, DefaultBNQuantizeConfig())\n",
    "    \n",
    "    return layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "                    model,\n",
    "                    clone_function=apply_quantization_to_batch_normalization,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with quantize_scope(\n",
    "  {'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "quant_aware_model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "model_train = quant_aware_model\n",
    "output_weights_name='FP_32_QAT_weights.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "             output_weights_name,\n",
    "             save_weights_only=True,\n",
    "             save_best_only=True,\n",
    "             verbose=1,\n",
    "            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow.keras.backend as kb\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shutil\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "class MultipleClassAUROC(Callback):\n",
    "    \"\"\"\n",
    "    Monitor mean AUROC and update model\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, class_names, weights_path, stats=None):\n",
    "        super(Callback, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.class_names = class_names\n",
    "        self.weights_path = weights_path\n",
    "        self.best_weights_path = os.path.join(\n",
    "            os.path.split(weights_path)[0],\n",
    "            f\"best_{os.path.split(weights_path)[1]}\",\n",
    "        )\n",
    "        self.best_auroc_log_path = os.path.join(\n",
    "            os.path.split(weights_path)[0],\n",
    "            \"best_auroc.log\",\n",
    "        )\n",
    "        self.stats_output_path = os.path.join(\n",
    "            os.path.split(weights_path)[0],\n",
    "            \".training_stats.json\"\n",
    "        )\n",
    "        # for resuming previous training\n",
    "        if stats:\n",
    "            self.stats = stats\n",
    "        else:\n",
    "            self.stats = {\"best_mean_auroc\": 0}\n",
    "\n",
    "        # aurocs log\n",
    "        self.aurocs = {}\n",
    "        for c in self.class_names:\n",
    "            self.aurocs[c] = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Calculate the average of the ROC Curves and save the best group of weights\n",
    "        according to this metric\n",
    "        \"\"\"\n",
    "        print(\"\\n*********************************\")\n",
    "        self.stats[\"lr\"] = float(kb.eval(self.model.optimizer.lr))\n",
    "        print(f\"Learning Rate actual: {self.stats['lr']}\")\n",
    "\n",
    "        \"\"\"\n",
    "        y_hat shape: (#examples, len(labels))\n",
    "        y: [(#examples, 1), (#examples, 1) ... (#examples, 1)]\n",
    "        \"\"\"\n",
    "        y_hat = self.model.predict_generator(self.generator,steps=self.generator.n/self.generator.batch_size)\n",
    "        y = self.generator.labels\n",
    "\n",
    "        print(f\"*** epoch#{epoch + 1} ROC Curves Training Phase ***\")\n",
    "        current_auroc = []\n",
    "        for i in range(len(self.class_names)):\n",
    "            try:\n",
    "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
    "            except ValueError:\n",
    "                score = 0\n",
    "            self.aurocs[self.class_names[i]].append(score)\n",
    "            current_auroc.append(score)\n",
    "            print(f\"{i+1}. {self.class_names[i]}: {score}\")\n",
    "        print(\"*********************************\")\n",
    "\n",
    "        mean_auroc = np.mean(current_auroc)\n",
    "        print(f\"Average ROC Curves: {mean_auroc}\")\n",
    "        if mean_auroc > self.stats[\"best_mean_auroc\"]:\n",
    "            print(f\"Update of the result of the ROC Curves of: {self.stats['best_mean_auroc']} a {mean_auroc}\")\n",
    "\n",
    "            # 1. copy best model\n",
    "            shutil.copy(self.weights_path, self.best_weights_path)\n",
    "\n",
    "            # 2. update log file\n",
    "            print(f\"Update log files: {self.best_auroc_log_path}\")\n",
    "            with open(self.best_auroc_log_path, \"a\") as f:\n",
    "                f.write(f\"(epoch#{epoch + 1}) auroc: {mean_auroc}, lr: {self.stats['lr']}\\n\")\n",
    "\n",
    "            # 3. write stats output, this is used for resuming the training\n",
    "            with open(self.stats_output_path, 'w') as f:\n",
    "                json.dump(self.stats, f)\n",
    "\n",
    "            print(f\"Weight group update {self.weights_path} -> {self.best_weights_path}\")\n",
    "            self.stats[\"best_mean_auroc\"] = mean_auroc\n",
    "            print(\"*********************************\")\n",
    "        return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_stats = {}\n",
    "auroc = MultipleClassAUROC(\n",
    "    generator=valid_gen,\n",
    "    class_names=all_labels,\n",
    "    weights_path=output_weights_name,\n",
    "    stats=training_stats\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "initial_learning_rate=1e-3\n",
    "optimizer = Adam(lr=initial_learning_rate)\n",
    "model_train.compile(optimizer=optimizer, loss=\"binary_crossentropy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "logs_base_dir = os.getcwd()\n",
    "patience_reduce_lr=2\n",
    "min_lr=1e-8\n",
    "callbacks = [\n",
    "            checkpoint,\n",
    "            TensorBoard(log_dir=os.path.join(logs_base_dir, \"logs\"), batch_size=train_gen.batch_size),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
    "                              verbose=1, mode=\"min\", min_lr=min_lr),\n",
    "            auroc,\n",
    "        ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs=20\n",
    "fit_history = model_train.fit_generator(\n",
    "    generator=train_gen,\n",
    "    steps_per_epoch=train_gen.n/train_gen.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=valid_gen.n/valid_gen.batch_size,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize = (15,8)) \n",
    "    \n",
    "plt.subplot(222)  \n",
    "plt.plot(fit_history.history['loss'])  \n",
    "plt.plot(fit_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'valid']) \n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_y = model_train.predict_generator(test_gen, steps=test_gen.n/test_gen.batch_size, verbose = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "test_gen.reset()\n",
    "test_x, test_y = next(test_gen)\n",
    "# Space\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
    "for (idx, c_label) in enumerate(all_labels):\n",
    "    #Points to graph\n",
    "    fpr, tpr, thresholds = roc_curve(test_gen.labels[:,idx].astype(int), pred_y[:,idx])\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    \n",
    "#convention\n",
    "c_ax.legend()\n",
    "\n",
    "#Labels\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "# Save as a png\n",
    "fig.savefig('QAT_FP32.png')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}